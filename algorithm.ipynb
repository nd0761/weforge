{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nd\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import codecs\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import n2w\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "import pickle\n",
    "import os\n",
    "import difflib\n",
    "import bisect\n",
    "import sys\n",
    "import cvxpy\n",
    "import scipy\n",
    "import random\n",
    "import re\n",
    "import hnswlib\n",
    "import string\n",
    "\n",
    "from necessary_funcs import *\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from iteration_utilities import unique_everseen\n",
    "from nltk.corpus import stopwords\n",
    "from geotext import GeoText\n",
    "from scipy.stats import norm\n",
    "from rdflib import Graph\n",
    "from networkx.readwrite import json_graph\n",
    "from sklearn.metrics import jaccard_score\n",
    "from os.path import expanduser\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import asarray as ar,exp\n",
    "from scipy.special import factorial\n",
    "from scipy.stats import expon\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from copy import deepcopy\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Pay special attention to__:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vectype: _(vals: 'glove','w2v'), type of vector you're using_  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sample_text_name: _name of the patent_  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dloads: _full patent filename_  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bin_lower, bin_upper: _the bins within histogram of TFIDF values_  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_fkdoc: _number of fake docs_  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;threshold_f, mu: _reference the paper_  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lower, upper: _bounds on number of fake document, ref. the paper_  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;window_size: _size of the context window around the unknown word_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening up patent  ./patents_html/US20120245375A1_good.txt\n"
     ]
    }
   ],
   "source": [
    "DIST_OPTION = 'Euclid'\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "sample_text_name = 'US20120245375A1'#random.sample(names, 1)[0] #'US2996519A'\n",
    "\n",
    "\n",
    "dloads = './patents_html/' + sample_text_name + '_good.txt'\n",
    "\n",
    "print('opening up patent ', dloads)\n",
    "\n",
    "name = 'en'\n",
    "# nlp = spacy.load('en')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "         \n",
    "bool_repeat = False#bool(sys.argv[0])\n",
    "\n",
    "bin_lower = 2#binss[0] # 3\n",
    "bin_upper = 18#binss[1] # 9\n",
    "\n",
    "\n",
    "bin_range = (bin_lower - 1, bin_upper + 1)\n",
    "\n",
    "\n",
    "n_fkdoc = 3#int(sys.argv[1])\n",
    "threshold_f = 1.0# for explicit optim\n",
    "threshold_r = 1. # for implicit optim\n",
    "\n",
    "lower = 2\n",
    "upper = 70\n",
    "mu = 2\n",
    "window_size = 10\n",
    "vectype = 'w2v'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Open the patent txt file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(dloads, 'r') as f:\n",
    "    labelText = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following cell loads variety of helper objects, their names implying their function\n",
    "\n",
    "\n",
    "\n",
    "__weforge__: string; name of the folder which contains all of the helper files  \n",
    "__cluster_to_vocab_index__: dict;_mapping from the cluster id to word vector vocabulary indices of words within the cluster (e.g. 2000 -> [1, 13, 52])_  \n",
    "__inx_to_vectors__: dict; _vocab index to vector value (e.g. 99 -> [0.5 -0.32 ... 1])_  \n",
    "__inx_to_word__: dict;_vocab index to a word (e.g. 101 -> 'backbone')_  \n",
    "__centroids__: dict; numpy array of all centroid values  \n",
    "__cluster_assignments__: list; _array cluster ids; each value is index by vocabulary position_  \n",
    "__unsorted_vocab__: list; _regular, unsorted word vector vocabulary (keep in mind that GloVe and Word2Vec have different vocabs due to different tokenization schemes)_  \n",
    "__sorted_vocab_indices__: list; _array of integers that represent sorted order of unsorted_vocab ( NOTE: we don't want to store both sorted and unsorted vocabulary, but we also don't want to sort the vocabulary at fake doc generation time, so this is a compromise solution)_  \n",
    "__transform_matrix__: np.array; 300 x 300 matrix that maps context vector into regular word vector space  \n",
    "__tfidf__: TfidfVectorizer object; learned on the corpus (by calling .fit(corpus)). Contains both idf vocabulary (since TfidfVectorizer tokenizes text itself, so it might differ from word vector vocabulary) and the idf values themselves. NOTE: you have to compute tf values yourself and then multiply them by idf values.  \n",
    "__neighbors__: np.array; array of neighbors for each word, that is a 2d matrix of dimensionality (VOCAB_SIZE x NUM_NEIGHBORS)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nd\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.0 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nd\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.0 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "weforge = './weforge_new/'\n",
    "# mapping:\n",
    "# cluster_id --> [list of indexes of words in the cluster]\n",
    "cluster_to_vocab_index = pload(weforge, vectype + '_cluster_to_indices')\n",
    "\n",
    "# mapping:\n",
    "# index --> [the word vector]\n",
    "\n",
    "inx_to_vectors = pload(weforge, vectype +  '_vocab_inx_to_vector')\n",
    "inx_to_word = pload(weforge, vectype + '_vocab_inx_to_word')\n",
    "\n",
    "# LOAD DICTIONARY FROM WORD TO INDEX IN GLOVE\n",
    "word_to_index = pload(weforge, vectype + '_word_to_inx')\n",
    "# neighbors = pload(weforge, vectype + '_neighbors')\n",
    "# ca - centroids\n",
    "centroids = pload(weforge, vectype + '_ca')\n",
    "# array:\n",
    "# glove cluster_assignments [cluster_id_1 ... cluster_(num_unique_words)]\n",
    "# inx                      [0            ...  num_unique_words]\n",
    "cluster_assignments = pload(weforge, vectype + '_assignments')\n",
    "\n",
    "# load sorted vocabulary to find insertion point for a word in the document\n",
    "\n",
    "\n",
    "unsorted_vocab = pload(weforge, vectype + '_vocab')\n",
    "sorted_vocab_indices = pload(weforge, vectype + '_sorted_vocab_indices')\n",
    "\n",
    "\n",
    "sorted_vec_vocab = [unsorted_vocab[ii] for ii in sorted_vocab_indices]\n",
    "\n",
    "transform_matrix = pload(weforge  + '/training_set/',  vectype + '_transform_matrix')\n",
    "\n",
    "\n",
    "cluster_to_indices = pload(weforge, vectype + '_cluster_to_indices')\n",
    "\n",
    "tfidf = pload(weforge, '/tfidf_object')\n",
    "\n",
    "neighbors = pload(weforge, vectype + '_neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load actual vectors\n",
    "Notice that we look at last the 300 values and not values from 1 to 301. That's because some words are composed of multiple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if vectype == 'glove':\n",
    "    vectors = []\n",
    "    with open(weforge + '/glove_subset_vector.txt', 'r') as f:\n",
    "        for ii, i in enumerate(tqdm(f)):\n",
    "            z = i.strip().split()\n",
    "            zz = [float(tok) for tok in z[-300:]]\n",
    "            vectors.append(zz)\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "else:\n",
    "    w2v = Word2Vec.load(weforge + '/model_subset.bin')\n",
    "    vectors = w2v.wv.vectors # extract vectors from the model\n",
    "    # word2index = {token: w2v.wv.vocab.get(token).index for token in w2v.wv.vocab}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process patent text: convert into the form we can work with\n",
    "  \n",
    "1. Break down entire patent into sequence of sentences.\n",
    "2. Tag parts of speech within each sentence.  \n",
    "3. Break down text into one long continuous sequence of tokens.    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;_e.g. [['He', 'ran', 'away' '.'], ['It', 'was', 'unbearable', '.']] turns into ['He', 'ran', 'away', '.', 'It', 'was', 'unbearable' ,'.']_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# # nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "#####\n",
    "#####   TEST TEXT PROCESSING\n",
    "#####\n",
    "##############################\n",
    "sentences = nltk.sent_tokenize(labelText)\n",
    "\n",
    "word_tokens = []\n",
    "for sent in sentences:\n",
    "    \n",
    "    x = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    word_tokens.extend(x)\n",
    "\n",
    "\n",
    "execute = True\n",
    "\n",
    "noun_loc = {}\n",
    "noun_map = {}\n",
    "\n",
    "\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\"-\", \"\") # don't remove hyphens\n",
    "table = str.maketrans('', '', remove)\n",
    "stop_words = set(stopwords.words('english') + ['thereof', 'wherein']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep the nouns\n",
    "\n",
    "Filter out every part of the speech except for the nouns. Then, lowercase them and remove all of the punctuation except hyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if execute:\n",
    "    p_nouns = []\n",
    "    for i in range(len(word_tokens)):\n",
    "        if (word_tokens[i][1] == 'NN'): \n",
    "            if word_tokens[i][0] not in stop_words:\n",
    "                p_nouns.append(word_tokens[i][0])\n",
    "\n",
    "                if word_tokens[i][0] in noun_loc:\n",
    "                    noun_loc[word_tokens[i][0]].append(i)\n",
    "                else:\n",
    "                    noun_loc[word_tokens[i][0]] = [i]\n",
    "                    \n",
    "    noun_list = []\n",
    "    for i in p_nouns:\n",
    "\n",
    "        p = deepcopy(i)\n",
    "\n",
    "        i = i.strip() #remove leading and trailing spaces\n",
    "        i = i.translate(table)\n",
    "\n",
    "        if len(i) > 1:\n",
    "            noun_list.append(i.lower())\n",
    "\n",
    "            noun_map[i.lower()] = p\n",
    "\n",
    "    # unique nouns in the list\n",
    "    noun_set = set(noun_list)\n",
    "    noun_set = list(noun_set)\n",
    "\n",
    "else:\n",
    "    if execute:\n",
    "        with open(home + '/noun_set_' + name + '.pkl', 'rb') as f:\n",
    "            noun_set = pickle.load(f)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "full_noun_set = deepcopy(noun_set)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF computation: step 1\n",
    "\n",
    "\n",
    "1. Use CountVectorizer object to compute term frequencies\n",
    "2. Find out the tokenization that CountVectorizer performed, i.e. load the vocabulary \n",
    "3. Match this vocabulary to the IDF vocabulary, as there might be words mispelled in the patent, or words not found in the original corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Level 1: Count frequency of terms within document of interest\n",
    "# Level 2: Pass document as a single text (has to be within a list) \n",
    "count = CountVectorizer()\n",
    "label_text_as_document = [labelText]\n",
    "\n",
    "counts = count.fit(label_text_as_document)\n",
    "doc_vocab = counts.vocabulary_\n",
    "sorted_doc_vocab = sorted(doc_vocab.keys())\n",
    "term_counts = (count.fit_transform(label_text_as_document)).todense() # call dense to convert sparse matrix to normal numpy array\n",
    "\n",
    "\n",
    "# np.array of shape (N, )\n",
    "idf_vals = tfidf.idf_\n",
    "\n",
    "idf_corpus_vocab = list(tfidf.vocabulary_)\n",
    "tfidf_vocab_sorted_indices = pload(weforge, '/tfidf_vocab_sorted_indices')\n",
    "tfidf_sorted_vocab = [idf_corpus_vocab[ii] for ii in tfidf_vocab_sorted_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF computation: a digression\n",
    "\n",
    "Before we proceed with actual computation of tf-idf values, we need to define a function that will filter out nouns that have low IDF values in a random corpus. Why? By definition low IDF implies high document frequency, which in turn implies that these words are representative of this random corpus. Thus, these words' discriminative value for our metallurgy corpus is low and we need to remove them from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 190 ['wt', 'performance', 'increase', 'carbon', 'stirring', 'example', 'process', 'completion', 'intermediate', 'meterability', 'need', 'olefin', 'addition', 'decomposition', 'feed', 'wash', 'preference', 'proportion', 'temperature', 'aldehyde', 'analysis', 'protonation', 'risk', 'monoester', 'equivalent', 'oxide', 'derivative', 'cycloaddition', 'method', 'ozonide', 'use', 'potassium', 'position', 'trickle', 'oxygen', 'group', 'core', 'processing', 'exchange', 'context', 'advantage', 'problem', 'object', 'instance', 'mlmin', 'embodiment', 'oxidizing', 'discharge', 'system', 'manner', 'chain', 'removal', 'field', 'azelate', 'production', 'priority', 'effect', 'news', 'turn', 'peroxide', 'ester', 'water', 'conversion', 'comprising', 'solvent', 'reduction', 'air', 'avoidance', 'batchwise', 'condenser', 'substrate', 'dioxide', 'formation', 'conc', 'sulphide', 'heat', 'react', 'alkyl', 'concentration', 'oleate', 'option', 'aspect', 'scheme', 'safer', 'reactor', 'inlet', 'hydrogenium', 'pka', 'countercurrent', 'preparation', 'establishment', 'ozone', 'bath', 'order', 'bond', 'alpha', 'oil', 'principle', 'form', 'fragment', 'preferably', 'equilibrium', 'combination', 'acceleration', 'product', 'relative', 'volume', 'signal', 'generator', 'reaction', 'step', 'cleavage', 'downstream', 'methyl', 'acetone', 'carboxylic', 'introduction', 'ketone', 'hydrogen', 'power', 'adduct', 'alcohol', 'amount', 'mode', 'type', 'flask', 'figure', 'decarboxylation', 'entirety', 'mechanism', 'starting', 'catalysis', 'disclosure', 'carbonyl', 'degradation', 'tube', 'weight', 'recombine', 'disadvantage', 'offgas', 'elimination', 'case', 'application', 'succession', 'number', 'result', 'apparatus', 'summary', 'gas', 'hydroperoxide', 'ed', 'purpose', 'agent', 'mixture', 'section', 'range', 'triphenylphosphine', 'understanding', 'bed', 'time', 'content', 'reference', 'percent', 'overall', 'sequence', 'herein', 'stream', 'area', 'isopropanol', 'catalyst', 'pure', 'workup', 'bur', 'rate', 'illustration', 'ozonolysis', 'monomethyl', 'place', 'polymeric', 'region', 'compound', 'acid', 'invention', 'solution', 'carboxyl', 'description', 'action', 'possibility', 'oxidation', 'ion']\n",
      "after 69 ['process', 'meterability', 'olefin', 'feed', 'protonation', 'risk', 'monoester', 'derivative', 'cycloaddition', 'use', 'potassium', 'oxygen', 'core', 'processing', 'context', 'mlmin', 'embodiment', 'oxidizing', 'manner', 'chain', 'removal', 'priority', 'turn', 'ester', 'water', 'conversion', 'avoidance', 'batchwise', 'dioxide', 'react', 'oleate', 'scheme', 'safer', 'reactor', 'pka', 'countercurrent', 'order', 'alpha', 'oil', 'form', 'fragment', 'acceleration', 'volume', 'reaction', 'acetone', 'adduct', 'flask', 'decarboxylation', 'entirety', 'degradation', 'weight', 'recombine', 'offgas', 'succession', 'apparatus', 'summary', 'hydroperoxide', 'section', 'triphenylphosphine', 'time', 'content', 'percent', 'stream', 'isopropanol', 'pure', 'ozonolysis', 'monomethyl', 'carboxyl', 'ion']\n"
     ]
    }
   ],
   "source": [
    "# random_tfidf = pload(weforge, '/random_tfidf_object')\n",
    "# random_idf_vals = random_tfidf.idf_\n",
    "\n",
    "# random_idf_corpus_vocab = list(random_tfidf.vocabulary_)\n",
    "# random_tfidf_vocab_sorted_indices = pload(weforge, '/random_tfidf_vocab_sorted_indices')\n",
    "# random_tfidf_sorted_vocab = [random_idf_corpus_vocab[ii] for ii in random_tfidf_vocab_sorted_indices]\n",
    "\n",
    "# entries, bin_middles, bin_edges = hgram(random_idf_vals)\n",
    "\n",
    "\n",
    "\n",
    "# random_left_edge = bin_edges[7]\n",
    "# random_right_edge = bin_edges[-1]\n",
    "def find_nonmatching_words_for_list(noun_list, sorted_vocab):\n",
    "    l = []\n",
    "    matches = {}\n",
    "    for noun in noun_list:\n",
    "        loc = find_insertion_point(noun, sorted_vocab)\n",
    "        left_bound = bound_aware_op(loc - 1, 0, len(sorted_vocab))\n",
    "        right_bound = bound_aware_op(loc + 1, 0, len(sorted_vocab))\n",
    "        matches[noun] = difflib.get_close_matches(noun, [sorted_vocab[left_bound], sorted_vocab[right_bound]], cutoff=0.8)\n",
    "        if len(matches[noun]) == 0:\n",
    "            l.append(noun)\n",
    "    return l\n",
    "\n",
    "\n",
    "print('before', len(noun_set), noun_set)\n",
    "\n",
    "bad_words = pload(weforge, '/random_tfidf_sorted_words_2')\n",
    "noun_set = [x for x in noun_set if x not in bad_words]\n",
    "\n",
    "noun_set = find_nonmatching_words_for_list(noun_set, bad_words)\n",
    "\n",
    "\n",
    "\n",
    "print('after', len(noun_set),noun_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF computation: step 2\n",
    "\n",
    "Next step in computing tf-idf is figuring out what words in the patent of interest are present in the corpus vocabulary found by TfIdfVectorizer(). Likely, we won't find exact matches, so we settle for the ones that are above certain threshold (Levenstein distance of 0.6).\n",
    "\n",
    "\n",
    "Similarly, find matches within CountVectorizer tokenized vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:00<00:00, 17249.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'process': 1, 'meterability': 1, 'olefin': 1, 'feed': 1, 'protonation': 1, 'risk': 1, 'monoester': 1, 'derivative': 1, 'cycloaddition': 1, 'use': 1, 'potassium': 1, 'oxygen': 1, 'core': 1, 'processing': 1, 'context': 1, 'mlmin': 1, 'embodiment': 1, 'oxidizing': 1, 'manner': 1, 'chain': 1, 'removal': 1, 'priority': 1, 'turn': 1, 'ester': 1, 'water': 1, 'conversion': 1, 'avoidance': 1, 'batchwise': 1, 'dioxide': 1, 'react': 1, 'oleate': 1, 'scheme': 1, 'safer': 1, 'reactor': 1, 'pka': 1, 'countercurrent': 1, 'order': 1, 'alpha': 1, 'oil': 1, 'form': 1, 'fragment': 1, 'acceleration': 1, 'volume': 1, 'reaction': 1, 'acetone': 1, 'adduct': 1, 'flask': 1, 'decarboxylation': 1, 'entirety': 1, 'degradation': 1, 'weight': 1, 'recombine': 1, 'offgas': 1, 'succession': 1, 'apparatus': 1, 'summary': 1, 'hydroperoxide': 1, 'section': 1, 'triphenylphosphine': 1, 'time': 1, 'content': 1, 'percent': 1, 'stream': 1, 'isopropanol': 1, 'pure': 1, 'ozonolysis': 1, 'monomethyl': 1, 'carboxyl': 1, 'ion': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "#####\n",
    "#####   FIND MATCHES OF NOUNS IN THE TEST DOCUMENT\n",
    "#####   IN COUNT VECTORIZER TOKENIZED TEXT\n",
    "#####   IN IDF TOKENIZED CORPUS\n",
    "#####\n",
    "##############################\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tfidf_matches = {}\n",
    "idf_matches = {}\n",
    "\n",
    "print(Counter(noun_set))\n",
    "for noun in tqdm(noun_set):\n",
    "    tfidf_matches[noun] = find_matching_word(noun, tfidf_sorted_vocab)\n",
    "    idf_matches[noun] = find_matching_word(noun, sorted_doc_vocab)\n",
    "    \n",
    "#     print(noun, tfidf_matches[noun])\n",
    "#     print(noun, idf_matches[noun])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tfidf_vals = []\n",
    "indices_that_survive = []\n",
    "unknown = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF computation: step 3\n",
    "\n",
    "Finally, compute tf-idf based on two following factors:\n",
    "1. Whether we found a match in idf vocab\n",
    "2. Whether we found a match in CountVectorizer vocab\n",
    "\n",
    "If a word fails one of these conditions we remove it from consideration.\n",
    "\n",
    "Once we computed tf-idf values we call a function called hgram to bin values. Once we do that we can filter out words whose tf-idf value is the range outside of the one specified by the hyperparams: bin_lower, bin_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:00<00:00, 71124.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_edges [0.0015077  0.00680859 0.01210947 0.01741036 0.02271125 0.02801213\n",
      " 0.03331302 0.03861391 0.04391479 0.04921568 0.05451657 0.05981745\n",
      " 0.06511834 0.07041923 0.07572011 0.081021   0.08632189 0.09162277\n",
      " 0.09692366 0.10222454 0.10752543]\n",
      "indices_that_survive 68 tfidf_vals 68\n",
      "[noun_set[i] for i in indices_that_survive] ['process', 'meterability', 'olefin', 'feed', 'protonation', 'risk', 'monoester', 'derivative', 'cycloaddition', 'use', 'potassium', 'oxygen', 'core', 'processing', 'context', 'embodiment', 'oxidizing', 'manner', 'chain', 'removal', 'priority', 'turn', 'ester', 'water', 'conversion', 'avoidance', 'batchwise', 'dioxide', 'react', 'oleate', 'scheme', 'safer', 'reactor', 'pka', 'countercurrent', 'order', 'alpha', 'oil', 'form', 'fragment', 'acceleration', 'volume', 'reaction', 'acetone', 'adduct', 'flask', 'decarboxylation', 'entirety', 'degradation', 'weight', 'recombine', 'offgas', 'succession', 'apparatus', 'summary', 'hydroperoxide', 'section', 'triphenylphosphine', 'time', 'content', 'percent', 'stream', 'isopropanol', 'pure', 'ozonolysis', 'monomethyl', 'carboxyl', 'ion']\n",
      "first_bin ['meterability', 'olefin', 'feed', 'protonation', 'risk', 'monoester', 'potassium', 'core', 'processing', 'context', 'manner', 'chain', 'removal', 'priority', 'turn', 'conversion', 'avoidance', 'batchwise', 'dioxide', 'react', 'safer', 'reactor', 'countercurrent', 'order', 'oil', 'fragment', 'acceleration', 'volume', 'acetone', 'adduct', 'flask', 'decarboxylation', 'entirety', 'degradation', 'weight', 'recombine', 'succession', 'apparatus', 'summary', 'section', 'triphenylphosphine', 'time', 'content', 'percent', 'stream', 'isopropanol', 'pure', 'monomethyl', 'ion'] first_bin_tfidf_vals [0.004526110280089101, 0.0023435932408047313, 0.004857600126158871, 0.0033544036187897045, 0.002601620172291053, 0.0032544472792025918, 0.0036740814518319595, 0.002216270710397393, 0.0018337029652934667, 0.00480883472721754, 0.0037938749188169106, 0.00456430527894017, 0.00330375252178517, 0.0025661594862253856, 0.002432266161382207, 0.0018716538673262896, 0.0033601697206301975, 0.005528329954338644, 0.0052361050399502285, 0.004353567282460067, 0.0033042214735284742, 0.004386745567686388, 0.002670700790085823, 0.0017094739345948295, 0.004075880298519456, 0.0029688076869422795, 0.00312916558237735, 0.003722904389531155, 0.004386730990323746, 0.005501343315606978, 0.004503281007530682, 0.003165246963856076, 0.0025207502293869205, 0.002440251560561321, 0.006347477093762406, 0.003930105536628225, 0.0032095014832193174, 0.0018114256415973247, 0.00275421510211475, 0.0019655012369232364, 0.003071958994260367, 0.0029942742163264185, 0.002918447594975674, 0.004057443021517398, 0.004264710733879453, 0.0025749856970466787, 0.003789071192774157, 0.006136808343374869, 0.0015077011173182727]\n",
      "tfidf_vals ['process', 'derivative', 'cycloaddition', 'use', 'oxygen', 'embodiment', 'oxidizing', 'ester', 'water', 'oleate', 'scheme', 'pka', 'alpha', 'form', 'reaction', 'offgas', 'hydroperoxide', 'carboxyl']\n",
      "tfidf_vals [0.021867442959116978, 0.014230579423224404, 0.007128471670777098, 0.015441921452125902, 0.011517628634392832, 0.0110108223073272, 0.019487899619589726, 0.010883167850105931, 0.015321478603626608, 0.00998107020090246, 0.018405063901873374, 0.00876417523848235, 0.02008766711190541, 0.009386233990908498, 0.05703087023033333, 0.008798407753605348, 0.011140138574880915, 0.020970952129472725]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "<ipython-input-12-e0bd8f5b26e4>:62: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "#####\n",
    "#####   COMPUTE TF-IDF\n",
    "#####\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "for i, noun in enumerate(tqdm(noun_set)):\n",
    "    if len(idf_matches[noun]) > 0 and len(tfidf_matches[noun]) > 0:\n",
    "        tf_index = doc_vocab[idf_matches[noun][0]]\n",
    "        tf_val = term_counts[0,tf_index]/np.sum(term_counts)\n",
    "\n",
    "        idf_vocab_index = tfidf.vocabulary_[tfidf_matches[noun][0]]\n",
    "        idf_val = idf_vals[idf_vocab_index]\n",
    "\n",
    "        tfidf_vals.append(tf_val*idf_val)\n",
    "\n",
    "        indices_that_survive.append(i)\n",
    "    else:\n",
    "        if len(noun) > 2:\n",
    "            unknown.append(noun)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tfidf_vals = np.array(tfidf_vals)\n",
    "entries, bin_middles, bin_edges = hgram(tfidf_vals)\n",
    "\n",
    "# BIN HYPERPARAMS\n",
    "\n",
    "\n",
    "left_edge = bin_edges[bin_range[0]]\n",
    "right_edge = bin_edges[bin_range[1]]\n",
    "\n",
    "\n",
    "first_left_edge = bin_edges[0]\n",
    "first_right_edge = bin_edges[1]\n",
    "\n",
    "\n",
    "\n",
    "print('bin_edges', bin_edges)\n",
    "print('indices_that_survive', len(indices_that_survive), 'tfidf_vals', len(tfidf_vals))\n",
    "noun_set = [noun_set[i] for i in indices_that_survive]\n",
    "print('[noun_set[i] for i in indices_that_survive]', noun_set)\n",
    "\n",
    "first_bin = [noun for i, noun in enumerate(noun_set) if tfidf_vals[i] >= first_left_edge and tfidf_vals[i] <= first_right_edge] \n",
    "first_bin_tfidf_vals = [tfidf_vals[i] for i, noun in enumerate(noun_set) if tfidf_vals[i] >= first_left_edge and tfidf_vals[i] <= first_right_edge]\n",
    "plt.clf()\n",
    "\n",
    "print('first_bin', first_bin, 'first_bin_tfidf_vals', first_bin_tfidf_vals)\n",
    "\n",
    "first_entries, first_bin_middles, first_bin_edges = hgram(first_bin_tfidf_vals)\n",
    "\n",
    "fbe_len = len(first_bin_edges)\n",
    "\n",
    "fbe_first = first_bin_edges[fbe_len - 6]\n",
    "fbe_last = first_bin_edges[fbe_len - 1]\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "noun_set = list(noun_set)\n",
    "\n",
    "\n",
    "# Remove the words that do not have TFIDF values in proper range\n",
    "# Remove the words that do not have a match in corpus vocab\n",
    "# noun_set = [noun_set[i] for i in indices_that_survive]\n",
    "\n",
    "noun_set = [(noun,tfidf_vals[i])  for i, noun in enumerate(noun_set) if tfidf_vals[i] >= left_edge and tfidf_vals[i] <= right_edge] \n",
    "#temp = [tfidf_vals[i] for i, noun in enumerate(noun_set) if tfidf_vals[i] >= left_edge and tfidf_vals[i] <= right_edge]\n",
    "\n",
    "cpy = deepcopy(noun_set)\n",
    "\n",
    "\n",
    "noun_set = [x[0] for x in cpy]\n",
    "tfidf_vals =[x[1] for x in cpy]\n",
    "# add_noun_set = [noun for i, noun in enumerate(first_bin) if first_bin_tfidf_vals[i] >= fbe_first and first_bin_tfidf_vals[i] <= fbe_last]\n",
    "\n",
    "# print('old noun_set  ', noun_set,add_noun_set)\n",
    "\n",
    "# noun_set = noun_set + add_noun_set\n",
    "\n",
    "# tfidf_vals = tfidf_vals + first_bin_tfidf_vals\n",
    "\n",
    "# print(' noun_set  ', noun_set)\n",
    "\n",
    "##############################\n",
    "#####\n",
    "#####   \n",
    "#####\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "print('tfidf_vals', noun_set)\n",
    "\n",
    "print('tfidf_vals', tfidf_vals)\n",
    "# once you found where the word of interest lies, use 0.6 levenstein distance cutoff to figure out if \n",
    "# there is a close match\n",
    "matches = find_matching_words_for_list(noun_set, sorted_vec_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ##############################\n",
    "# #####\n",
    "# #####   KNOWN WORDS PROCESSING:\n",
    "# #####   1. \n",
    "# #####   2.\n",
    "# ##### \n",
    "# #####\n",
    "# ##############################\n",
    "clusters_of_interest = []\n",
    "cluster_to_noun = {}\n",
    "nn_dist = {}\n",
    "nn_word = {}\n",
    "\n",
    "\n",
    "\n",
    "max_seq_len = 0\n",
    "noun_to_util_index = {}\n",
    "lens = []\n",
    "\n",
    "\n",
    "# # ADD\n",
    "# # BIN FILTERING \n",
    "nn_vals = []\n",
    "noun_counter = 0 \n",
    "indices_that_survive = []\n",
    "r_a = {}\n",
    "noun_to_neighbors = {}\n",
    "nouns_that_survive = {}\n",
    "counter = 0\n",
    "\n",
    "nn_dist_implicit = {}\n",
    "noun_to_neighbors_implicit = {}\n",
    "nouns_that_survive_implicit = {}\n",
    "nn_vals_implicit = []\n",
    "counter_implicit = 0\n",
    "noun_counter_implicit = 0\n",
    "max_seq_len_implicit = 0\n",
    "lens_implicit = []\n",
    "indices_that_survive_implicit = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop which goes thru relevant nouns (the ones that survived tf-idf bin filtering) and finds their nearest neighbors (removes the ones with the same stem as the noun to be replaced) and computes the NN distances (because K-Means/K-NN code doesn't provide distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process method\n",
      "True True\n",
      "derivative cycloheteroalkyl\n",
      "True True\n",
      "cycloaddition aminoketone\n",
      "True True\n",
      "cycloaddition 5-amino-34-dihydro-1\n",
      "True True\n",
      "cycloaddition 5-nitro-34-dihydro-1\n",
      "True True\n",
      "cycloaddition ring-closure\n",
      "True True\n",
      "cycloaddition aromatisation\n",
      "True True\n",
      "cycloaddition -naphthylimine\n",
      "True True\n",
      "cycloaddition -naphthalenone\n",
      "True True\n",
      "use utilize\n",
      "True True\n",
      "use employ\n",
      "True True\n",
      "use us\n",
      "True True\n",
      "embodiment aspect\n",
      "True True\n",
      "oxidizing absorbs\n",
      "True True\n",
      "oxidizing oxidise\n",
      "True True\n",
      "water bittern\n",
      "True True\n",
      "water seawater\n",
      "True True\n",
      "water saline\n",
      "True True\n",
      "oleate 2-ethyl\n",
      "True True\n",
      "oleate xanthate\n",
      "True True\n",
      "oleate carboxymethylcellulose\n",
      "True True\n",
      "oleate carboxymethyl\n",
      "True True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 130.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha beta\n",
      "True True\n",
      "offgas off-gases\n",
      "True True\n",
      "offgas offgases\n",
      "True True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# print(unknown_word_neighbors)\n",
    "    \n",
    "    \n",
    "# word_to_index\n",
    "# cluster_assignments\n",
    "# cluster_to_vocab_index\n",
    "# neighbors\n",
    "\n",
    "indices_that_survive = []\n",
    "# FOR EVERY RELEVANT NOUN WE FIND THE NEIGHBORS THAT ARE NOT IN THE DOCUMENT AND WE ALSO COMPUTE DISTANCE BETWEEN THIS WORD AND EVERY NEIGHBOR\n",
    "for i, noun in enumerate(tqdm(noun_set)):\n",
    "    #nn_vals.append([])\n",
    "    if len(matches[noun]) > 0:\n",
    "        nn_dist[noun] = []\n",
    "         # 2d list that will be turned to costs matrix using numpy \n",
    "        noun_to_util_index[noun] = i # dic from noun to its index in the matrix that will be used in optimization\n",
    "        word_index = word_to_index[matches[noun][0]]\n",
    "        cluster_id = cluster_assignments[word_index]\n",
    "        clusters_of_interest.append(cluster_id)\n",
    "\n",
    "        # if statement that exists only to fill in \n",
    "        # the cluster_to_noun \n",
    "        if cluster_id not in cluster_to_noun:\n",
    "            cluster_to_noun[cluster_id] = [noun]\n",
    "        else:\n",
    "            cluster_to_noun[cluster_id].append(noun)\n",
    "\n",
    "        # confusing naming: this step returns all other members of the cluster current word 'noun'\n",
    "        # belongs to \n",
    "        other_members = cluster_to_vocab_index[cluster_id]\n",
    "        ngbrs = neighbors[word_index]\n",
    "        ngbrs = [xx for xx in ngbrs if xx != word_index]\n",
    "        \n",
    "\n",
    "        # BUG: ordering(distances) are not preserved by the set operations \n",
    "        x_implicit = set(ngbrs).intersection(set(other_members)) # set of all nearest neighbors who are also in the cluster\n",
    "        x_implicit = x_implicit.difference(noun_set)\n",
    "        \n",
    "        x = set(ngbrs)\n",
    "        x = x.difference(noun_set) # set of all nearest neighbors who are not in the document \n",
    "        \n",
    "        neighbor_order = {}\n",
    "        neighbor_order_implicit = {}\n",
    "        \n",
    "        \n",
    "        num_neighbors_for_cur_noun = len(x)\n",
    "        num_neighbors_for_cur_noun_implicit = len(x_implicit)\n",
    "        \n",
    "        ###########\n",
    "        # EXPLICIT COMPUTATION\n",
    "        ###########\n",
    "\n",
    "        # set difference might come up empty\n",
    "        if num_neighbors_for_cur_noun > 0:\n",
    "            nouns_that_survive[counter] = noun\n",
    "            nn_vals.append([])\n",
    "\n",
    "            if num_neighbors_for_cur_noun > max_seq_len:\n",
    "                max_seq_len = num_neighbors_for_cur_noun\n",
    "                \n",
    "            # for each word in a neighbor set \n",
    "            # find its ordering within older list of neighbors (as a proxy for proximity)\n",
    "            nn_dist[noun] = []\n",
    "            dist_run_sum = 0\n",
    "            order_of_indices = []\n",
    "            noun_to_neighbors[noun] = []\n",
    "            ngbrs = []\n",
    "\n",
    "            \n",
    "            for j, neighbor in enumerate(x):\n",
    "                if word_index != neighbor and stemmer.stem(noun) != stemmer.stem(inx_to_word[neighbor]):\n",
    "                    neighbor_order[neighbor] = np.where(ngbrs == neighbor)[0]\n",
    "                    order_of_indices.append(neighbor_order[neighbor])\n",
    "                    ngbrs.append(neighbor)\n",
    "\n",
    "\n",
    "                    # compute L2 dist between the noun of interest and its neighbor\n",
    "                    # HYPERPARAM 'ord', for Manhattan 1, Euclidian 2\n",
    "                    if DIST_OPTION == 'Manhattan' or DIST_OPTION == 'Euclid':\n",
    "                        dist = np.linalg.norm(np.array(inx_to_vectors[word_index]) -  np.array(inx_to_vectors[neighbor]), ord=2)\n",
    "                    else:\n",
    "                        dist  = scipy.spatial.distance.cosine(np.array(inx_to_vectors[word_index]), np.array(inx_to_vectors[neighbor]))\n",
    "\n",
    "                    # nn_dist[noun].append((word_to_index[neighbor], neighbor_order[neighbor], dist))\n",
    "                    nn_vals[noun_counter].append(dist)\n",
    "                    dist_run_sum += dist\n",
    "                else:\n",
    "                    num_neighbors_for_cur_noun = num_neighbors_for_cur_noun - 1\n",
    "        \n",
    "            \n",
    "            noun_to_neighbors[noun] = ngbrs\n",
    "\n",
    "            lens.append(num_neighbors_for_cur_noun)\n",
    "            noun_counter += 1\n",
    "            indices_that_survive.append(i)\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "        ###########\n",
    "        # IMPLICIT COMPUTATION\n",
    "        ###########\n",
    "        if num_neighbors_for_cur_noun_implicit > 0:\n",
    "    \n",
    "            nouns_that_survive_implicit[counter] = noun\n",
    "            nn_vals_implicit.append([])\n",
    "\n",
    "            if num_neighbors_for_cur_noun_implicit > max_seq_len_implicit:\n",
    "                max_seq_len_implicit = num_neighbors_for_cur_noun_implicit\n",
    "                \n",
    "\n",
    "            \n",
    "            nn_dist_implicit[noun] = []\n",
    "            dist_run_sum_implicit = 10e-42\n",
    "            order_of_indices_implicit = []\n",
    "            noun_to_neighbors_implicit[noun] = []\n",
    "            ngbrs_implicit = []\n",
    "\n",
    "            for j, neighbor_implicit in enumerate(x_implicit):\n",
    "                print(noun, inx_to_word[neighbor_implicit])\n",
    "                print(word_index != neighbor_implicit, stemmer.stem(noun) != stemmer.stem(inx_to_word[neighbor_implicit]))\n",
    "                \n",
    "                \n",
    "\n",
    "                if word_index != neighbor_implicit and stemmer.stem(noun) != stemmer.stem(inx_to_word[neighbor_implicit]):\n",
    "                    neighbor_order_implicit[neighbor_implicit] = np.where(ngbrs_implicit == neighbor_implicit)[0]\n",
    "                    order_of_indices_implicit.append(neighbor_order_implicit[neighbor_implicit])\n",
    "                    ngbrs_implicit.append(neighbor_implicit)\n",
    "\n",
    "                    # compute L2 dist between the noun of interest and its neighbor\n",
    "                    # HYPERPARAM 'ord', for Manhattan 1, Euclidian 2\n",
    "                    if DIST_OPTION == 'Manhattan' or DIST_OPTION == 'Euclid':\n",
    "                        dist_implicit = np.linalg.norm(np.array(inx_to_vectors[word_index]) -  np.array(inx_to_vectors[neighbor_implicit]), ord=2)\n",
    "                    else:\n",
    "                        dist_implicit  = scipy.spatial.distance.cosine(np.array(inx_to_vectors[word_index]), np.array(inx_to_vectors[neighbor_implicit]))\n",
    "\n",
    "                    # nn_dist[noun].append((word_to_index[neighbor], neighbor_order[neighbor], dist))\n",
    "                    nn_vals_implicit[noun_counter_implicit].append(dist_implicit)\n",
    "                    \n",
    "                    dist_run_sum_implicit += dist_implicit\n",
    "                else:\n",
    "                    num_neighbors_for_cur_noun_implicit = num_neighbors_for_cur_noun_implicit - 1\n",
    "    \n",
    "\n",
    "#             r_a[noun] = (len(xx)**2)/dist_run_sum_implicit\n",
    "            r_a[noun] = (len(noun_to_neighbors_implicit)**2)/dist_run_sum_implicit\n",
    "            noun_to_neighbors_implicit[noun] = ngbrs_implicit\n",
    "            lens_implicit.append(num_neighbors_for_cur_noun_implicit)\n",
    "            indices_that_survive_implicit.append(i)\n",
    "            noun_counter_implicit += 1\n",
    "            \n",
    "\n",
    "            counter_implicit += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fast way to create costs mask \n",
    "# print(lens)\n",
    "maxlen = max(lens)\n",
    "costs = np.zeros((len(nn_vals),maxlen),int)\n",
    "mask = np.arange(maxlen) < np.array(lens)[:,None] # key line\n",
    "costs[mask] = np.concatenate(nn_vals)    # fast 1d assignment\n",
    "\n",
    "tfidf_vals = np.array(tfidf_vals)\n",
    "relevant_tf_idf_vals = tfidf_vals[indices_that_survive]\n",
    "utilities = np.diag(relevant_tf_idf_vals) # tf-idf values or \\xi sign in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization equation\n",
    "\n",
    "$$\\min \\sum\\limits_{f\\in\\mathcal{F}}\\sum\\limits_{c\\in d}\\sum\\limits_{c'\\in \\mathcal{FC}(c,d)} ||v_c-v_{c'}||\\cdot \\xi_c\\cdot X_{c,c',f}$$\n",
    "$$$$\n",
    "$$\\text{subject to}\\,\\,\\, (i) \\,\\,\\,  \\sum\\limits_{f\\in \\mathcal{F}}\\sum\\limits_{c'\\in \\mathcal{FC}(c,d)}X_{c,c',f}\\leq \\mu, \\forall c\\in d$$\n",
    "$$$$\n",
    "$$(ii) \\,\\,\\, \\sum\\limits_{c'\\in \\mathcal{FC}(c,d)}X_{c,c',f}\\leq 1, \\forall c\\in d,f\\in \\mathcal{F}$$\n",
    "$$$$\n",
    "$$(iii) \\,\\,\\, N_l\\leq \\sum\\limits_{c\\in d}\\sum\\limits_{c'\\in \\mathcal{FC}(c,d)}X_{c,c',f} \\leq N_u, \\ \\forall f\\in \\mathcal{F}$$\n",
    "$$$$\n",
    "$$(iv) \\,\\,\\, \\sum\\limits_{c\\in d}\\sum\\limits_{c'\\in\\mathcal{FC}(c,d)}||v_c-v_{c'}||X_{c,c',f}\\geq T_f, \\ \\forall f\\in \\mathcal{F}$$\n",
    "$$$$\n",
    "$$(v)\\,\\,\\, X_{c,c',f}=0,\\ \\forall c\\notin \\mathcal{B}$$\n",
    "$$$$\n",
    "$$(vi)\\,\\,\\, X_{c,c',f} \\in \\{0,1\\}, \\ \\forall c,c',f$$\n",
    "\n",
    "\n",
    "\n",
    "$X_{c,c',f}\\in\\{0,1\\}$ is a binary variable, where $X_{c,c',f}$=1 means a concept $c\\in d$ will be replaced by a candidate $c'\\in\\mathcal{FC}(c,d)$ to generate a fake document $f\\in\\mathcal{F}$. We explicitly encode the concept replacement in the joint optimization problem. Constraint (i) indicates that a concept can be replaced at most $\\mu$ times. Constraint (ii) restricts that the number of concepts that are going to be replaced in each fake documentris bounded by $N_l$ and $N_u$. Constraint (iii) means that for each fake document $f \\in \\mathcal{F}$, the overall replacement availability should not be smaller than a threshold $T_f$ (so that it will be able to be notably different from the original document to disguise important information). Constraint (iv) is such that the overall distance of concepts and their replacements is constrained to be equal or larger than a customer-specified threshold $T_f$. In addition, we add another constraint (ii) to enforce that each fake document $f$ and each concept $c$, at most one of the concepts in the feasible candidate set $\\mathcal{FC}(c,d)$ is used to replace the target concept $c$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cvxopt in c:\\users\\nd\\anaconda3\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install cvxopt\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global list_of_concepts\n",
    "list_of_concepts = []\n",
    "\n",
    "\n",
    "\n",
    "num_concepts = len(nn_vals)#len(outputs)\n",
    "full_selection = {}\n",
    "for i in range(n_fkdoc):\n",
    "    full_selection[i] = cvxpy.Variable((num_concepts, max_seq_len), boolean=True)\n",
    "\n",
    "\n",
    "ones = np.ones((1, max_seq_len))\n",
    "\n",
    "replacement_constraint = []\n",
    "\n",
    "\n",
    "num_concepts_to_be_replaced_constraint = []\n",
    "dist_constraint = []\n",
    "\n",
    "for i in range(n_fkdoc):\n",
    "    replacement_constraint.append(cvxpy.sum(full_selection[i], 1) <= 1)\n",
    "    dist_constraint.append(cvxpy.sum(cvxpy.multiply(full_selection[i], costs))  >= threshold_f)\n",
    "    num_concepts_to_be_replaced_constraint.append(cvxpy.sum(full_selection[i]) <= upper)\n",
    "    num_concepts_to_be_replaced_constraint.append(lower <=  cvxpy.sum(full_selection[i]))\n",
    "\n",
    "    #num_times_you_can_replace_concept_across_fake_docs = num_times_you_can_replace_concept_across_fake_docs #+  full_selection[i]\n",
    "\n",
    "\n",
    "num_times_you_can_replace_concept_across_fake_docs = full_selection[0]\n",
    "for i in range(1, n_fkdoc):\n",
    "    num_times_you_can_replace_concept_across_fake_docs += full_selection[i]\n",
    "\n",
    "\n",
    "num_times_you_can_replace_concept_across_fake_docs = cvxpy.sum(num_times_you_can_replace_concept_across_fake_docs, 1)\n",
    "mu_cstrt = np.ones((len(nn_vals)))*mu\n",
    "nconstraint = [num_times_you_can_replace_concept_across_fake_docs <= mu]\n",
    "\n",
    "total_utility = 0\n",
    "for i in range(n_fkdoc):\n",
    "    # (N x N) x (N x max_seq_len) =  (N x max_seq_len)\n",
    "    weighted_costs = utilities.dot(costs)\n",
    "\n",
    "    #  (max_seq_len x N)  x (N x max_seq_len) x (max_seq_len x 1)\n",
    "    total_utility += cvxpy.sum(cvxpy.multiply(full_selection[i],weighted_costs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_constraints = replacement_constraint  + num_concepts_to_be_replaced_constraint + dist_constraint + nconstraint\n",
    "\n",
    "knapsack_problem = cvxpy.Problem(cvxpy.Maximize(total_utility),  all_constraints)\n",
    "\n",
    "# knapsack_problem.solve()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate fake docs thru explicit optimization done above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find a solution for explicit optimization\n"
     ]
    }
   ],
   "source": [
    "fake_docs_explicit = []\n",
    "\n",
    "lst_target = []\n",
    "\n",
    "lst_sub = []\n",
    "\n",
    "\n",
    "if all([full_selection[i].value is not None for i in range(n_fkdoc)]):\n",
    "\n",
    "    for i in range(n_fkdoc):\n",
    "        xx = np.round(full_selection[i].value)\n",
    "        print()\n",
    "\n",
    "        text1 = deepcopy(labelText)\n",
    "\n",
    "\n",
    "        print('FOR a FAKE DOC, EXPLICIT optimization', i+1)\n",
    "        for j in range(xx.shape[0]):\n",
    "            if len(np.nonzero(xx[j,:] > 0.9)[0]) > 0: \n",
    "                index = np.nonzero(xx[j,:] > 0.9)[0][0]\n",
    "\n",
    "                noun = nouns_that_survive[j]\n",
    "                list_of_neigbrs = noun_to_neighbors[noun]\n",
    "\n",
    "\n",
    "                replacement_index = list_of_neigbrs[index]\n",
    "\n",
    "                replacement = re.sub(r'\\W+', '', inx_to_word[replacement_index])\n",
    "                \n",
    "\n",
    "\n",
    "                print('replace', noun,'with', replacement)\n",
    "\n",
    "\n",
    "\n",
    "                target = noun\n",
    "\n",
    "\n",
    "\n",
    "                tmp1 = re.search('\\b' + target + '\\b', text1)\n",
    "                if tmp1 !=\"\":\n",
    "                    if(target[0].isupper()):\n",
    "                        replacement = replacement.capitalize()\n",
    "                    \n",
    "                    text1 = re.sub(r\"\\b%s\\b\" % target , replacement, text1)\n",
    "                    lst_target.append(target)\n",
    "                    lst_sub.append(replacement)\n",
    "                    \n",
    "                if(target[0].isupper() and not target[1].isupper() ):\n",
    "                        target = target.lower()  \n",
    "                        replacement = replacement.lower()\n",
    "                        if target in text1:\n",
    "                            text1 = re.sub(r\"\\b%s\\b\" % target , replacement, text1)\n",
    "\n",
    "                            lst_target.append(target)\n",
    "                            lst_sub.append(replacement)\n",
    "                        \n",
    "        if len(lst_target) > 0:\n",
    "            fake_docs_explicit.append(text1)\n",
    "\n",
    "    listt = 'explicit_fake_doc_no_' + str(i) + '_' + sample_text_name + '_blbulu_' + str(bin_lower)  + 'd' + str(bin_upper) + 'd' + str(lower)  + 'd' + str(upper) +  'w' + '_listtt'\n",
    "    \n",
    "    print('home', home)\n",
    "    print('listt', listt)\n",
    "    with open('./' + listt, 'w') as f:\n",
    "        for target, replacement in zip(lst_target, lst_sub):\n",
    "            f.write('replace' + ' ' + target + ' with ' + replacement + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "    for i, text in enumerate(fake_docs_explicit):\n",
    "        with open('./explicit_fake_doc_no_' + str(i) + '_' + sample_text_name + '_blbulu_' + str(bin_lower)  + 'd' + str(bin_upper) + 'd' + str(lower)  + 'd' + str(upper), 'w') as f:\n",
    "            f.write(text)\n",
    "else:\n",
    "    print('Didn\\'t find a solution for explicit optimization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up variables for implicit optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_concepts = len(nn_vals_implicit)#len(outputs)\n",
    "tfidf_vals = np.array(tfidf_vals)\n",
    "# CREATE COSTS MATRIX \n",
    "# THIS CODE IS CONFUSING BUT FAST FOR LARGE MATRIX VALUES\n",
    "# WHY? \n",
    "maxlen_implicit = max(lens_implicit)\n",
    "costs_implicit = np.zeros((len(nn_vals_implicit),maxlen_implicit),int)\n",
    "mask_implicit = np.arange(maxlen_implicit) < np.array(lens_implicit)[:,None] # key line\n",
    "costs_implicit[mask_implicit] = np.concatenate(nn_vals_implicit)    # fast 1d assignment\n",
    "\n",
    "relevant_tf_idf_vals_implicit = tfidf_vals[indices_that_survive_implicit]\n",
    "utilities_implicit = np.diag(relevant_tf_idf_vals_implicit) # tf-idf values or \\xi sign in the document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLICIT OPTIMIZATION\n",
    "\n",
    "$$\\min \\sum\\limits_{f\\in\\mathcal{F}}\\sum\\limits_{c\\in d}ra(c,d)\\cdot \\xi_c\\cdot X_{c,f}$$\n",
    "$$$$\n",
    "$$\\text{subject to}\\,\\,\\,(i)\\,\\,\\,\\sum\\limits_{f\\in \\mathcal{F}}X_{c,f}\\leq \\mu, \\forall c\\in d$$\n",
    "$$$$\n",
    "$$(ii)\\,\\,\\, N_l\\leq \\sum\\limits_{c\\in d}X_{c,f} \\leq N_u, \\ \\forall f\\in \\mathcal{F}$$\n",
    "$$$$\n",
    "$$(iii)\\,\\,\\, \\sum\\limits_{c\\in d}ra(c,d)X_{c,f}\\geq T_f, \\ \\forall f\\in \\mathcal{F}$$\n",
    "$$$$\n",
    "$$(iv)\\,\\,\\, X_{c,f}=0, \\forall c\\notin \\mathcal{B}$$\n",
    "$$$$\n",
    "$$\\,\\,\\,(v)\\,\\,\\, X_{c,f}\\in \\{0,1\\}, \\forall c,f$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1259994335290133"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit_selection_variable = cvxpy.Variable((num_concepts, n_fkdoc), boolean=True)\n",
    "\n",
    "implicit_num_concepts_for_all_fake_docs_constraint = [cvxpy.sum(implicit_selection_variable, 1) <= mu]\n",
    "\n",
    "implicit_num_concepts_per_fake_docs_constraint = [cvxpy.sum(implicit_selection_variable, 0) >= lower, cvxpy.sum(implicit_selection_variable, 0) <= upper]\n",
    "\n",
    "\n",
    "\n",
    "implicit_ra_multiplier = np.array(list(r_a.values()))\n",
    "\n",
    "implicit_ra_multiplier = np.reshape(implicit_ra_multiplier, (num_concepts, 1))\n",
    "\n",
    "implicit_ra_multiplier = np.tile(implicit_ra_multiplier, (1, n_fkdoc))\n",
    "\n",
    "implicit_ra_constraint = [cvxpy.sum(cvxpy.multiply(implicit_selection_variable, implicit_ra_multiplier), 0) >= threshold_r]\n",
    "\n",
    "\n",
    "\n",
    "all_implicit_constraints = implicit_num_concepts_per_fake_docs_constraint + implicit_num_concepts_for_all_fake_docs_constraint + implicit_ra_constraint\n",
    "\n",
    "\n",
    "implicit_utility = cvxpy.sum(cvxpy.multiply(implicit_selection_variable, np.dot(utilities_implicit, implicit_ra_multiplier)))\n",
    "implicit_knapsack_problem = cvxpy.Problem(cvxpy.Maximize(implicit_utility),  all_implicit_constraints)\n",
    "\n",
    "implicit_knapsack_problem.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate fake docs using implicit optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR a FAKE DOC, implicit optimization 1\n",
      "replace process with methanold4\n",
      "replace cycloaddition with cyclotriacontane\n",
      "replace use with emulsifier\n",
      "replace oxygen with peroxocarbonate\n",
      "replace embodiment with 2carboxybenzaldehyde\n",
      "replace oxidizing with withdraws\n",
      "replace ester with decarboxylation\n",
      "replace water with wastewater\n",
      "replace oleate with hexanoate\n",
      "FOR a FAKE DOC, implicit optimization 2\n",
      "replace process with tetrafluorosilane\n",
      "replace derivative with multicoordination\n",
      "FOR a FAKE DOC, implicit optimization 3\n",
      "replace derivative with xny\n",
      "replace cycloaddition with dimerisation\n",
      "replace use with purchase\n",
      "replace oxygen with superoxide\n",
      "replace embodiment with eubacterium\n",
      "replace oxidizing with 3chloro2hydroxypropyl\n",
      "replace ester with octyl\n",
      "replace water with antiscalant\n",
      "replace oleate with 4hydroxybenzoate\n"
     ]
    }
   ],
   "source": [
    "lst_target = []\n",
    "lst_sub = []\n",
    "\n",
    "\n",
    "\n",
    "fake_docs_implicit = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if implicit_selection_variable.value is not None:\n",
    "    implicit_fk_concepts_val = np.round(implicit_selection_variable.value)\n",
    "\n",
    "\n",
    "    for i in range(n_fkdoc):\n",
    "\n",
    "        text1 = deepcopy(labelText)\n",
    "\n",
    "        xx = implicit_fk_concepts_val[:,i]\n",
    "        print('FOR a FAKE DOC, implicit optimization', i+1)\n",
    "\n",
    "        if len(np.nonzero(xx > 0.9)[0]) > 0: \n",
    "            indices = np.nonzero(xx > 0.9)[0]\n",
    "            for inx in indices:\n",
    "                noun = nouns_that_survive[inx]\n",
    "\n",
    "\n",
    "                list_of_neigbrs = noun_to_neighbors[noun]\n",
    "\n",
    "                index = random.randrange(0, len(list_of_neigbrs))\n",
    "\n",
    "                replacement_index = list_of_neigbrs[index]\n",
    "\n",
    "                replacement = re.sub(r'\\W+', '', inx_to_word[replacement_index])\n",
    "\n",
    "                target = noun\n",
    "\n",
    "                tmp1 = re.search(r'\\b' + target + r'\\b', text1)\n",
    "                if tmp1 !=\"\":\n",
    "                    if(target[0].isupper()):\n",
    "                        replacement = replacement.capitalize()\n",
    "                    \n",
    "                    text1 = re.sub(r\"\\b%s\\b\" % target , replacement, text1)\n",
    "                    lst_target.append(target)\n",
    "                    lst_sub.append(replacement)\n",
    "                    \n",
    "                if (target[0].isupper() and not target[1].isupper() ):\n",
    "                        target = target.lower()  \n",
    "                        replacement = replacement.lower()\n",
    "                        if target in text1:\n",
    "                            text1 = re.sub(r\"\\b%s\\b\" % target , replacement, text1)\n",
    "\n",
    "                            lst_target.append(noun)\n",
    "#                             lst_target.append(target)\n",
    "                            lst_sub.append(replacement)\n",
    "                        \n",
    "                print('replace', noun,'with', replacement)\n",
    "\n",
    "                \n",
    "                \n",
    "        if len(lst_target) > 0:\n",
    "            fake_docs_implicit.append(text1)\n",
    "\n",
    "\n",
    "    listt = 'implicit_fake_doc_no_' + str(i) + '_' + sample_text_name + '_blbulu_' + str(bin_lower)  + 'd' + str(bin_upper) + 'd' + str(lower)  + 'd' + str(upper) + 'w' + '_listtt'\n",
    "\n",
    "\n",
    "    with open('./' + listt, 'w') as f:\n",
    "        for target, replacement in zip(lst_target, lst_sub):\n",
    "            f.write('replace' + ' ' + target + ' with ' + replacement + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i, text in enumerate(fake_docs_implicit):\n",
    "        with open('./' + 'implicit_fake_doc_no_' + str(i) + '_' + sample_text_name + '_blbulu_' + str(bin_lower)  + 'd' + str(bin_upper) + 'd' + str(lower)  + 'd' + str(upper), 'w') as f:\n",
    "            f.write(text)\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Didn\\'t find a solution for implicit optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
